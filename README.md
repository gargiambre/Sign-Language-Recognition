# Sign-Language-Recognition
Real-Time Sign Language Recognition (RTSLG) employs technology to convert hand motions into text or voice as they are signed. This help us in bridging the communication gap between the deaf and hearing communities. Hands carry a rich tapestry of information, allowing people with disabilities to express themselves meaningfully while improving communication for everyone. However, precisely understanding these transient hand gestures in real time presents a challenge. This is an important challenge for the hearing-impaired group, which often depends on hand signals to convey their ideas and feelings yet obtains minimal understanding from the general public. Recognizing this important necessity, hand gesture recognition technologies have gained popularity in recent years. Our analysis of the literature recognized limitations in existing sign language recognition approaches. Some gestures have been eliminated from their datasets, particularly dynamic ones, restricting their ability to record these subtle movements. Additionally, these systems often battled with accuracy as a result of differences in the background lighting. To address these restrictions, we created a unique mechanism that solves them.The suggested architecture uses a deep learning model created using Python, TensorFlow, OpenCV, and Keras. This RTSLG system analyzes motions of the hand collected by a recording device using image detection, computer vision, and, in particular, Convolutional Neural Networks (CNNs). We were able to recognize and transform motions into text with an outstanding  accuracy of 95\% after deploying our technique.
